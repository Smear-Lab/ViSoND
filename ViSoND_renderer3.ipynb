{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d47e2-3114-4fa4-9cbb-4692dd80cc58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d2887-8f43-424f-9fde-bdd2bc8f680c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f94f63-1b26-447b-b0dc-640826522bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689681b579b0446dbefff816c09ce520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Load Kilosort files</b>'), HBox(children=(FileUpload(value=(), accept='.npy', de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ViSoND: Build MIDI matrix from Kilosort with rate filters + RPV filter + (optional) similarity ordering\n",
    "\n",
    "import io, os, numpy as np, pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, FileLink\n",
    "\n",
    "# ---------- UI ----------\n",
    "times_upload   = widgets.FileUpload(accept='.npy', multiple=False, description='spike_times.npy')\n",
    "clust_upload   = widgets.FileUpload(accept='.npy', multiple=False, description='spike_clusters.npy')\n",
    "\n",
    "times_path = widgets.Text(description='Times path', placeholder='spike_times.npy', layout=widgets.Layout(width='48%'))\n",
    "clust_path = widgets.Text(description='Clusters path', placeholder='spike_clusters.npy', layout=widgets.Layout(width='48%'))\n",
    "\n",
    "sr = widgets.IntText(value=30000, description='Sample rate (Hz)')\n",
    "times_in_samples = widgets.Checkbox(value=True, description='Times are in samples')\n",
    "\n",
    "spike_dur = widgets.FloatText(value=0.01, description='Spike duration (s)')\n",
    "vel = widgets.IntSlider(value=100, min=1, max=127, description='Velocity')\n",
    "track_id = widgets.IntText(value=0, description='Track ID')\n",
    "channel  = widgets.IntSlider(value=0, min=0, max=15, description='MIDI Channel')\n",
    "\n",
    "min_rate_hz = widgets.FloatText(value=0.1, description='Min rate Hz')\n",
    "max_rate_hz = widgets.FloatText(value=0.0, description='Max rate Hz (0=off)')\n",
    "rpv_ms      = widgets.FloatText(value=2.0, description='Refractory (ms)')\n",
    "max_rpv_frac= widgets.FloatText(value=0.05, description='Max RPV frac')\n",
    "top_k_clusters = widgets.IntText(value=0, description='Keep top-K (0=all)')\n",
    "\n",
    "order_dd = widgets.Dropdown(\n",
    "    options=[\n",
    "        'first appearance',\n",
    "        'rate high→low (most active = lowest)',\n",
    "        'rate low→high (most active = highest)',\n",
    "        'cluster id ascending',\n",
    "        'cluster id descending',\n",
    "        'similarity: cosine',\n",
    "        'similarity: EMD',\n",
    "    ],\n",
    "    value='similarity: cosine',\n",
    "    description='Note order',\n",
    "    layout=widgets.Layout(width='320px')\n",
    ")\n",
    "bin_s = widgets.FloatText(value=0.100, description='Bin (s) for similarity')\n",
    "\n",
    "build_btn = widgets.Button(description='Build MIDI Matrix', button_style='success')\n",
    "out = widgets.Output()\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<b>Load Kilosort files</b>\"),\n",
    "    widgets.HBox([times_upload, clust_upload]),\n",
    "    widgets.HBox([times_path, clust_path]),\n",
    "    widgets.HBox([sr, times_in_samples]),\n",
    "    widgets.HBox([spike_dur, vel]),\n",
    "    widgets.HBox([track_id, channel]),\n",
    "    widgets.HBox([min_rate_hz, max_rate_hz, top_k_clusters]),\n",
    "    widgets.HBox([rpv_ms, max_rpv_frac]),\n",
    "    widgets.HBox([order_dd, bin_s]),\n",
    "    build_btn,\n",
    "    widgets.HTML(\"<b>Output</b>\"),\n",
    "    out\n",
    "]))\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _get_upload_bytes(upl):\n",
    "    if not upl.value: return None, None\n",
    "    v = upl.value\n",
    "    if isinstance(v, dict):  # ipywidgets v7\n",
    "        name, info = next(iter(v.items()))\n",
    "        return name, info['content']\n",
    "    else:  # ipywidgets v8\n",
    "        f = v[0]\n",
    "        name = getattr(f, 'name', None) or f.get('name')\n",
    "        content = getattr(f, 'content', None) or f.get('content')\n",
    "        return name, content\n",
    "\n",
    "def _load_npy_from_upload_or_path(upl, path_text, label):\n",
    "    name, content = _get_upload_bytes(upl)\n",
    "    if content:\n",
    "        try:\n",
    "            return np.load(io.BytesIO(content), allow_pickle=False), name\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load {label} from upload: {e}\")\n",
    "    p = (path_text.value or \"\").strip()\n",
    "    if not p:\n",
    "        raise ValueError(f\"No {label} uploaded or path provided.\")\n",
    "    if not os.path.exists(p):\n",
    "        raise ValueError(f\"{label} path not found: {p}\")\n",
    "    return np.load(p, allow_pickle=False), p\n",
    "\n",
    "def _binned_matrix(times_s, cluster_ids, bin_seconds):\n",
    "    t0, t1 = float(times_s.min()), float(times_s.max())\n",
    "    duration = max(1e-9, t1 - t0)\n",
    "    nbins = max(1, int(np.ceil(duration / max(1e-6, float(bin_seconds)))))\n",
    "    edges = np.linspace(t0, t1, nbins+1, endpoint=True)\n",
    "    uniq = np.unique(cluster_ids)\n",
    "    M = np.zeros((uniq.size, nbins), dtype=np.float32)\n",
    "    for i, cid in enumerate(uniq):\n",
    "        ts = times_s[cluster_ids == cid]\n",
    "        hist, _ = np.histogram(ts, bins=edges)\n",
    "        M[i, :] = hist\n",
    "    return uniq, edges, M\n",
    "\n",
    "def _cosine_distance_matrix(X):\n",
    "    X = X.astype(np.float64)\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True); norms[norms == 0] = 1.0\n",
    "    Y = X / norms\n",
    "    S = np.clip(Y @ Y.T, -1.0, 1.0)\n",
    "    D = 1.0 - S\n",
    "    np.fill_diagonal(D, 0.0)\n",
    "    return D\n",
    "\n",
    "def _emd_distance_matrix(X, bin_edges):\n",
    "    X = X.astype(np.float64)\n",
    "    row_sums = X.sum(axis=1, keepdims=True); row_sums[row_sums == 0] = 1.0\n",
    "    P = X / row_sums\n",
    "    C = np.cumsum(P, axis=1)\n",
    "    n = C.shape[0]\n",
    "    D = np.zeros((n, n), dtype=np.float64)\n",
    "    bw = np.diff(bin_edges)\n",
    "    w = float(np.mean(bw)) if bw.size else 1.0\n",
    "    for i in range(n):\n",
    "        diff = np.abs(C[i][None, :] - C)\n",
    "        D[i, :] = w * diff.sum(axis=1)\n",
    "    np.fill_diagonal(D, 0.0)\n",
    "    return D\n",
    "\n",
    "def _order_by_similarity(times_s, cluster_ids, bin_seconds, method='cosine'):\n",
    "    uniq, edges, M = _binned_matrix(times_s, cluster_ids, bin_seconds)\n",
    "    D = _cosine_distance_matrix(M) if method == 'cosine' else _emd_distance_matrix(M, edges)\n",
    "    # 1-D classical MDS\n",
    "    J = np.eye(D.shape[0]) - np.ones(D.shape) / D.shape[0]\n",
    "    B = -0.5 * J @ (D**2) @ J\n",
    "    try:\n",
    "        vals, vecs = np.linalg.eigh(B)\n",
    "        idx = np.argsort(vals)[::-1]\n",
    "        y = vecs[:, idx[0]] * np.sqrt(max(vals[idx[0]], 0))\n",
    "    except Exception:\n",
    "        y = -D.mean(axis=1)\n",
    "    order = np.argsort(y)\n",
    "    return uniq[order].tolist()\n",
    "\n",
    "def _order_clusters(times_s, cluster_ids, strategy, bin_seconds=0.1):\n",
    "    s = pd.Series(cluster_ids)\n",
    "    uniq, counts = np.unique(s, return_counts=True)\n",
    "    dur = float(times_s.max() - times_s.min()) if times_s.size else 1.0\n",
    "    if dur <= 0: dur = 1.0\n",
    "    rates = counts / dur\n",
    "    strat = (strategy or '').lower()\n",
    "    if 'similarity: cosine' in strat:\n",
    "        return _order_by_similarity(times_s, cluster_ids, bin_seconds, method='cosine')\n",
    "    if 'similarity: emd' in strat:\n",
    "        return _order_by_similarity(times_s, cluster_ids, bin_seconds, method='emd')\n",
    "    if 'rate high→low' in strat or 'rate high' in strat:\n",
    "        dfm = pd.DataFrame({'id': uniq, 'rate': rates})\n",
    "        return dfm.sort_values(['rate','id'], ascending=[False, True])['id'].tolist()\n",
    "    if 'rate low→high' in strat or 'rate low' in strat:\n",
    "        dfm = pd.DataFrame({'id': uniq, 'rate': rates})\n",
    "        return dfm.sort_values(['rate','id'], ascending=[True, True])['id'].tolist()\n",
    "    if 'cluster id ascending' in strat:\n",
    "        return sorted(uniq.tolist())\n",
    "    if 'cluster id descending' in strat:\n",
    "        return sorted(uniq.tolist(), reverse=True)\n",
    "    first_idx = s.drop_duplicates(keep='first').index\n",
    "    first_vals = s.loc[first_idx].values\n",
    "    return [v for v in first_vals if v in uniq]\n",
    "\n",
    "def _compute_rpv_per_cluster(times_s, cluster_ids, refractory_s):\n",
    "    \"\"\"\n",
    "    Efficient RPV per cluster:\n",
    "    - Sort by (cluster, time), take ISIs within cluster\n",
    "    - RPV = (# ISIs < refractory_s) / (# ISIs) ; clusters with <2 spikes => RPV=0\n",
    "    Returns dict: cluster_id -> rpv_fraction\n",
    "    \"\"\"\n",
    "    order = np.lexsort((times_s, cluster_ids))  # primary: cluster, secondary: time\n",
    "    t_sorted = times_s[order]\n",
    "    c_sorted = cluster_ids[order]\n",
    "    isi = np.diff(t_sorted)\n",
    "    same = c_sorted[1:] == c_sorted[:-1]\n",
    "    isi_same = isi[same]\n",
    "    cluster_for_isi = c_sorted[1:][same]\n",
    "    # total ISIs per cluster\n",
    "    uniq_c, total_isi = np.unique(cluster_for_isi, return_counts=True)\n",
    "    # violation ISIs per cluster\n",
    "    vio_mask = isi_same < refractory_s\n",
    "    uniq_v, vio_counts = np.unique(cluster_for_isi[vio_mask], return_counts=True)\n",
    "    # build dict with defaults\n",
    "    rpv = {int(c): 0.0 for c in np.unique(cluster_ids)}\n",
    "    total_dict = dict(zip(uniq_c.astype(int), total_isi.astype(int)))\n",
    "    vio_dict = dict(zip(uniq_v.astype(int), vio_counts.astype(int)))\n",
    "    for c in rpv.keys():\n",
    "        tot = total_dict.get(c, 0)\n",
    "        if tot <= 0:\n",
    "            rpv[c] = 0.0\n",
    "        else:\n",
    "            rpv[c] = float(vio_dict.get(c, 0)) / float(tot)\n",
    "    return rpv\n",
    "\n",
    "def _build_matrix(times, clusters,\n",
    "                  sr_hz, times_are_samples,\n",
    "                  spike_dur_s, velocity, track, chan,\n",
    "                  min_rate, max_rate, rpv_window_ms, max_rpv, top_k, note_order, bin_seconds):\n",
    "    # Basic checks\n",
    "    times = np.asarray(times).squeeze()\n",
    "    clusters = np.asarray(clusters).squeeze()\n",
    "    if times.ndim != 1 or clusters.ndim != 1:\n",
    "        raise ValueError(\"Inputs must be 1D arrays.\")\n",
    "    if times.shape[0] != clusters.shape[0]:\n",
    "        raise ValueError(f\"Length mismatch: spike_times ({times.shape[0]}) vs spike_clusters ({clusters.shape[0]}).\")\n",
    "\n",
    "    # Convert times to seconds if needed\n",
    "    if times_are_samples:\n",
    "        if sr_hz <= 0:\n",
    "            raise ValueError(\"Sample rate must be positive when converting samples to seconds.\")\n",
    "        t_sec = times.astype(np.float64) / float(sr_hz)\n",
    "    else:\n",
    "        t_sec = times.astype(np.float64)\n",
    "\n",
    "    # Recording duration\n",
    "    rec_dur = float(np.max(t_sec) - np.min(t_sec)) if t_sec.size else 0.0\n",
    "    if rec_dur <= 0:\n",
    "        rec_dur = 1.0\n",
    "        rec_warn = True\n",
    "    else:\n",
    "        rec_warn = False\n",
    "\n",
    "    # Global per-cluster stats\n",
    "    uniq_all, counts_all = np.unique(clusters, return_counts=True)\n",
    "    rates_all = counts_all / rec_dur\n",
    "\n",
    "    # --- (1) Min/Max rate filters ---\n",
    "    keep = rates_all >= float(min_rate)\n",
    "    if max_rate and float(max_rate) > 0:\n",
    "        keep &= (rates_all <= float(max_rate))\n",
    "    clusters_rate_kept = uniq_all[keep]\n",
    "    counts_rate_kept   = counts_all[keep]\n",
    "\n",
    "    # Apply rate mask to spikes for RPV computation\n",
    "    mask_rate_spikes = np.isin(clusters, clusters_rate_kept)\n",
    "    t_rate = t_sec[mask_rate_spikes]\n",
    "    c_rate = clusters[mask_rate_spikes]\n",
    "\n",
    "    # --- (2) RPV filter ---\n",
    "    refr_s = float(rpv_window_ms) / 1000.0\n",
    "    rpv_dict = _compute_rpv_per_cluster(t_rate, c_rate, refr_s)\n",
    "    # keep clusters with RPV <= max_rpv\n",
    "    clusters_rpv_kept = [c for c in clusters_rate_kept if rpv_dict.get(int(c), 0.0) <= float(max_rpv)]\n",
    "    clusters_rpv_kept = np.array(clusters_rpv_kept, dtype=clusters_rate_kept.dtype)\n",
    "\n",
    "    # counts after RPV (for top-K)\n",
    "    if clusters_rpv_kept.size > 0:\n",
    "        mask_rpv_spikes = np.isin(clusters, clusters_rpv_kept)\n",
    "        _, counts_after_rpv = np.unique(clusters[mask_rpv_spikes], return_counts=True)\n",
    "    else:\n",
    "        counts_after_rpv = np.array([], dtype=int)\n",
    "\n",
    "    # --- (3) Top-K by count ---\n",
    "    kept_for_topk = clusters_rpv_kept\n",
    "    counts_for_topk = counts_after_rpv\n",
    "    if top_k and int(top_k) > 0 and kept_for_topk.size > int(top_k):\n",
    "        order_desc = np.argsort(-counts_for_topk)\n",
    "        kept_for_topk = kept_for_topk[order_desc[:int(top_k)]]\n",
    "\n",
    "    # Final spike mask\n",
    "    final_mask = np.isin(clusters, kept_for_topk)\n",
    "    t_final = t_sec[final_mask]\n",
    "    c_final = clusters[final_mask]\n",
    "\n",
    "    # Sort spikes by time (stable)\n",
    "    order = np.argsort(t_final, kind='mergesort')\n",
    "    t_final = t_final[order]\n",
    "    c_final = c_final[order]\n",
    "\n",
    "    # --- Ordering for note_id on kept spikes ---\n",
    "    ordered_clusters = _order_clusters(t_final, c_final, note_order, bin_seconds=bin_seconds)\n",
    "    rank_map = {cid: i for i, cid in enumerate(ordered_clusters)}\n",
    "    note_ids = np.array([rank_map[c] for c in c_final], dtype=int)\n",
    "\n",
    "    # Build matrix [track, channel, note_id, velocity, start_s, end_s]\n",
    "    n = t_final.shape[0]\n",
    "    M = np.empty((n, 6), dtype=object)\n",
    "    M[:, 0] = int(track)\n",
    "    M[:, 1] = int(chan)\n",
    "    M[:, 2] = note_ids\n",
    "    M[:, 3] = int(np.clip(velocity, 1, 127))\n",
    "    M[:, 4] = t_final\n",
    "    M[:, 5] = t_final + float(spike_dur_s)\n",
    "\n",
    "    # Summaries\n",
    "    n_after_rate = int(clusters_rate_kept.size)\n",
    "    n_after_rpv  = int(clusters_rpv_kept.size)\n",
    "    n_after_topk = int(len(np.unique(kept_for_topk)))\n",
    "    summary = {\n",
    "        \"clusters_total\":          int(len(uniq_all)),\n",
    "        \"clusters_after_rate\":     n_after_rate,\n",
    "        \"clusters_after_rpv\":      n_after_rpv,\n",
    "        \"clusters_after_topk\":     n_after_topk,\n",
    "        \"spikes_total\":            int(times.shape[0]),\n",
    "        \"spikes_after_filters\":    int(n),\n",
    "        \"recording_duration_s\":    float(rec_dur),\n",
    "        \"rec_duration_warning\":    rec_warn,\n",
    "        \"ordering\":                note_order,\n",
    "        \"bin_seconds\":             float(bin_seconds),\n",
    "        \"refractory_ms\":           float(rpv_window_ms),\n",
    "        \"max_rpv_frac\":            float(max_rpv)\n",
    "    }\n",
    "    # Per-cluster RPV (only for kept-after-rate clusters)\n",
    "    summary[\"rpv_examples\"] = {int(c): round(float(rpv_dict.get(int(c), 0.0)), 4) for c in clusters_rate_kept[:10]}\n",
    "    return M, summary, ordered_clusters\n",
    "\n",
    "def _preview_matrix(M, k=10):\n",
    "    df = pd.DataFrame(M, columns=[\"track\",\"channel\",\"note_id\",\"velocity\",\"start_s\",\"end_s\"])\n",
    "    display(df.head(k))\n",
    "    return df\n",
    "\n",
    "def _save_outputs(M, outstem=\"kilo_visond\"):\n",
    "    npy_path = f\"{outstem}.npy\"\n",
    "    csv_path = f\"{outstem}.csv\"\n",
    "    np.save(npy_path, M)\n",
    "    pd.DataFrame(M, columns=[\"track\",\"channel\",\"note_id\",\"velocity\",\"start_s\",\"end_s\"]).to_csv(csv_path, index=False)\n",
    "    return npy_path, csv_path\n",
    "\n",
    "def _build_clicked(_):\n",
    "    out.clear_output()\n",
    "    with out:\n",
    "        try:\n",
    "            times, t_name = _load_npy_from_upload_or_path(times_upload, times_path, \"spike_times.npy\")\n",
    "            clust, c_name = _load_npy_from_upload_or_path(clust_upload, clust_path, \"spike_clusters.npy\")\n",
    "            print(f\"Loaded times:    {t_name} shape={times.shape}\")\n",
    "            print(f\"Loaded clusters: {c_name} shape={clust.shape}\")\n",
    "\n",
    "            M, info, ordered_clusters = _build_matrix(\n",
    "                times, clust,\n",
    "                sr_hz=sr.value,\n",
    "                times_are_samples=times_in_samples.value,\n",
    "                spike_dur_s=spike_dur.value,\n",
    "                velocity=vel.value,\n",
    "                track=track_id.value,\n",
    "                chan=channel.value,\n",
    "                min_rate=min_rate_hz.value,\n",
    "                max_rate=max_rate_hz.value,\n",
    "                rpv_window_ms=rpv_ms.value,\n",
    "                max_rpv=max_rpv_frac.value,\n",
    "                top_k=top_k_clusters.value,\n",
    "                note_order=order_dd.value,\n",
    "                bin_seconds=bin_s.value\n",
    "            )\n",
    "\n",
    "            print(f\"\\nRecording duration: {info['recording_duration_s']:.3f} s\"\n",
    "                  + (\" [estimated]\" if info['rec_duration_warning'] else \"\"))\n",
    "            print(f\"Clusters: total → after rate → after RPV → after topK : \"\n",
    "                  f\"{info['clusters_total']} → {info['clusters_after_rate']} → {info['clusters_after_rpv']} → {info['clusters_after_topk']}\")\n",
    "            print(f\"Spikes:   total → after filters: {info['spikes_total']} → {info['spikes_after_filters']}\")\n",
    "            print(f\"RPV: window={info['refractory_ms']} ms, max frac={info['max_rpv_frac']}\")\n",
    "            if info.get(\"rpv_examples\"):\n",
    "                print(\"Sample RPV (first 10 clusters after rate filter):\")\n",
    "                for cid, r in info[\"rpv_examples\"].items():\n",
    "                    print(f\"  cluster {cid}: RPV={r}\")\n",
    "\n",
    "            print(f\"Ordering: {info['ordering']}  |  Bin: {info['bin_seconds']:.3f} s\")\n",
    "            print(\"First 12 cluster→note_id mappings:\")\n",
    "            for cid, nid in list({cid: i for i, cid in enumerate(ordered_clusters)}.items())[:12]:\n",
    "                print(f\"  cluster {cid} -> note_id {nid}\")\n",
    "\n",
    "            _preview_matrix(M, k=10)\n",
    "            npy_path, csv_path = _save_outputs(M, outstem=\"kilo_visond\")\n",
    "            print(\"\\nSaved:\")\n",
    "            display(FileLink(npy_path))\n",
    "            display(FileLink(csv_path))\n",
    "            print(\"\\nUse this .npy or .csv as input to the ViSoND render cell.\")\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "build_btn.on_click(_build_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c0aeed-eb8f-4faf-b6aa-d65f90e8a44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6167aa4bfa884e9685eb41811be41e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<b>Upload .csv or .npy (cols 0..5: track,channel,note,velocity,start_s,end_s)</b>')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UI ready: upload or path → set options → Render MIDI\n"
     ]
    }
   ],
   "source": [
    "# ViSoND one-shot UI — upload OR path + options + STRICT unique scale-ladder mapping + render\n",
    "# (Pitch ordering removed.)\n",
    "\n",
    "import io, numpy as np, pandas as pd, mido\n",
    "import ipywidgets as widgets\n",
    "from mido import MetaMessage\n",
    "from IPython.display import display, HTML, FileLink\n",
    "\n",
    "# ---------------- UI ----------------\n",
    "upload = widgets.FileUpload(accept='.csv,.npy', multiple=False)\n",
    "path_txt = widgets.Text(\n",
    "    description='Path',\n",
    "    placeholder='e.g., bigdata.npy (already in this folder)',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "bpm_slider = widgets.IntSlider(value=120, min=20, max=960, description='BPM')\n",
    "tpb_slider = widgets.IntSlider(value=5000, min=16, max=10000, description='Ticks/QN')\n",
    "root_dd = widgets.Dropdown(options=['C','C#','D','D#','E','F','F#','G','G#','A','A#','B'],\n",
    "                           value='C', description='Root')\n",
    "scale_dd = widgets.Dropdown(options=[], value=None, description='Scale', layout=widgets.Layout(width='300px'))\n",
    "render_btn = widgets.Button(description='Render MIDI', button_style='success')\n",
    "log = widgets.Output()\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<b>Upload .csv or .npy (cols 0..5: track,channel,note,velocity,start_s,end_s)</b>\"),\n",
    "    widgets.HBox([upload, path_txt]),\n",
    "    widgets.HBox([bpm_slider, tpb_slider]),\n",
    "    widgets.HBox([root_dd, scale_dd]),\n",
    "    render_btn,\n",
    "    widgets.HTML(\"<b>Log</b>\"),\n",
    "    log\n",
    "]))\n",
    "\n",
    "# ------------- Helpers & Scales -------------\n",
    "def _get_uploaded_name_and_bytes(upl):\n",
    "    v = upl.value\n",
    "    if not v: return None, None\n",
    "    # ipywidgets v7: dict; v8: tuple\n",
    "    if isinstance(v, dict):\n",
    "        name, info = next(iter(v.items()))\n",
    "        return name, info['content']\n",
    "    else:\n",
    "        f = v[0]\n",
    "        name = getattr(f, 'name', None) or f.get('name')\n",
    "        content = getattr(f, 'content', None) or f.get('content')\n",
    "        return name, content\n",
    "\n",
    "NOTE_TO_SEMITONE = {'C':0,'C#':1,'D':2,'D#':3,'E':4,'F':5,'F#':6,'G':7,'G#':8,'A':9,'A#':10,'B':11}\n",
    "\n",
    "# Extended scale set (12-TET approximations as needed)\n",
    "SCALE_INTERVALS = {\n",
    "    \"Chromatic (12-tone)\": list(range(12)),\n",
    "    \"Ionian (Major)\"            : [0, 2, 4, 5, 7, 9, 11],\n",
    "    \"Dorian\"                    : [0, 2, 3, 5, 7, 9, 10],\n",
    "    \"Phrygian\"                  : [0, 1, 3, 5, 7, 8, 10],\n",
    "    \"Lydian\"                    : [0, 2, 4, 6, 7, 9, 11],\n",
    "    \"Mixolydian\"                : [0, 2, 4, 5, 7, 9, 10],\n",
    "    \"Aeolian (Natural Minor)\"   : [0, 2, 3, 5, 7, 8, 10],\n",
    "    \"Locrian\"                   : [0, 1, 3, 5, 6, 8, 10],\n",
    "    \"Pentatonic Major\"                 : [0, 2, 4, 7, 9],\n",
    "    \"Pentatonic Minor\"                 : [0, 3, 5, 7, 10],\n",
    "    \"Pentatonic Egyptian\"              : [0, 2, 5, 7, 10],\n",
    "    \"Pentatonic Yo (Japanese)\"         : [0, 2, 5, 7, 9],\n",
    "    \"Pentatonic In Sen (Japanese)\"     : [0, 1, 5, 7, 10],\n",
    "    \"Pentatonic Hirajoshi (Japanese)\"  : [0, 2, 3, 7, 8],\n",
    "    \"Pentatonic Iwato (Japanese)\"      : [0, 1, 5, 6, 10],\n",
    "    \"Pentatonic Dorian\"                : [0, 2, 3, 7, 9],\n",
    "    \"Blues Minor (hexatonic)\" : [0, 3, 5, 6, 7, 10],\n",
    "    \"Blues Major (hexatonic)\" : [0, 2, 3, 4, 7, 9],\n",
    "    \"Chinese Gong (宫)\" : [0, 2, 4, 7, 9],\n",
    "    \"Chinese Shang (商)\": [0, 2, 5, 7, 10],\n",
    "    \"Chinese Jiao (角)\" : [0, 3, 5, 8, 10],\n",
    "    \"Chinese Zhi (徵)\"  : [0, 2, 5, 7, 9],\n",
    "    \"Chinese Yu (羽)\"   : [0, 3, 5, 7, 10],\n",
    "    \"Ethiopian Tezeta Major\": [0, 2, 4, 7, 9],\n",
    "    \"Ethiopian Tezeta Minor\": [0, 3, 5, 7, 10],\n",
    "    \"Ethiopian Bati Major\"  : [0, 2, 4, 7, 9],\n",
    "    \"Ethiopian Bati Minor\"  : [0, 2, 3, 7, 9],\n",
    "    \"Ethiopian Ambassel\"    : [0, 3, 5, 7, 10],\n",
    "    \"Ethiopian Anchihoye\"   : [0, 2, 3, 7, 9],\n",
    "}\n",
    "scale_dd.options = list(SCALE_INTERVALS.keys())\n",
    "scale_dd.value = \"Chromatic (12-tone)\"\n",
    "\n",
    "def build_allowed_pitches(root, scale, lo=0, hi=127):\n",
    "    root_pc = NOTE_TO_SEMITONE[root]\n",
    "    pattern = SCALE_INTERVALS[scale]\n",
    "    return np.array([n for n in range(lo, hi+1) if (n - root_pc) % 12 in pattern], dtype=int)\n",
    "\n",
    "def map_unique_to_scale_ladder_strict(values, root, scale, base_note=None):\n",
    "    \"\"\"\n",
    "    Order-preserving, 1:1 mapping into a scale ladder.\n",
    "    Raises ValueError if there are more unique values than capacity (0..127).\n",
    "    Uses first-appearance order; no pitch ordering UI here.\n",
    "    \"\"\"\n",
    "    vals = pd.Series(values)\n",
    "    uniq = vals.dropna().drop_duplicates().tolist()\n",
    "    nuniq = len(uniq)\n",
    "\n",
    "    allowed = build_allowed_pitches(root, scale, lo=0, hi=127)\n",
    "    if allowed.size == 0:\n",
    "        raise ValueError(f\"Scale '{scale}' with root '{root}' has no allowed pitches in 0–127.\")\n",
    "\n",
    "    start_idx = 0 if base_note is None else int(np.searchsorted(allowed, np.clip(int(base_note), 0, 127)))\n",
    "    capacity = max(0, allowed.size - start_idx)\n",
    "    if capacity <= 0:\n",
    "        start_idx = 0\n",
    "        capacity = allowed.size\n",
    "\n",
    "    if nuniq > capacity:\n",
    "        raise ValueError(f\"Too many notes: {nuniq} unique IDs, but only {capacity} pitches available \"\n",
    "                         f\"for scale '{scale}' in MIDI 0–127. Choose a larger scale or reduce uniqueness.\")\n",
    "\n",
    "    targets = allowed[start_idx:start_idx + nuniq]\n",
    "    mapping = dict(zip(uniq, targets))\n",
    "    return vals.map(mapping).fillna(60).astype(int).to_numpy()\n",
    "\n",
    "# ---------------- Render with progress bars ----------------\n",
    "def render_midi(_btn=None):\n",
    "    with log:\n",
    "        log.clear_output()\n",
    "        try:\n",
    "            # Load from upload or path\n",
    "            name, content = _get_uploaded_name_and_bytes(upload)\n",
    "            if content:\n",
    "                if str(name).lower().endswith('.csv'):\n",
    "                    df = pd.read_csv(io.BytesIO(content), header=None)\n",
    "                elif str(name).lower().endswith('.npy'):\n",
    "                    arr = np.load(io.BytesIO(content), allow_pickle=True)\n",
    "                    df = pd.DataFrame(arr)\n",
    "                else:\n",
    "                    print(\"Unsupported file type (use .csv or .npy).\"); return\n",
    "            else:\n",
    "                p = (path_txt.value or \"\").strip()\n",
    "                if not p:\n",
    "                    print(\"No file uploaded AND no path provided.\"); return\n",
    "                name = p\n",
    "                if p.lower().endswith('.csv'):\n",
    "                    df = pd.read_csv(p, header=None)\n",
    "                elif p.lower().endswith('.npy'):\n",
    "                    arr = np.load(p, allow_pickle=True)\n",
    "                    df = pd.DataFrame(arr)\n",
    "                else:\n",
    "                    print(\"Unsupported file type (use .csv or .npy).\"); return\n",
    "\n",
    "            if df.shape[1] < 6:\n",
    "                print(f\"Need ≥6 columns (track,channel,note,velocity,start_s,end_s). Got {df.shape[1]}.\"); return\n",
    "\n",
    "            # Clean columns\n",
    "            df = df.copy()\n",
    "            df[0] = pd.to_numeric(df[0], errors=\"coerce\").astype(\"Int64\")\n",
    "            df[1] = pd.to_numeric(df[1], errors=\"coerce\").fillna(0).clip(0,15).astype(int)\n",
    "            df[2] = pd.to_numeric(df[2], errors=\"coerce\").fillna(60).clip(0,127).astype(int)\n",
    "            df[3] = pd.to_numeric(df[3], errors=\"coerce\").fillna(64).clip(1,127).astype(int)\n",
    "            df[4] = pd.to_numeric(df[4], errors=\"coerce\")\n",
    "            df[5] = pd.to_numeric(df[5], errors=\"coerce\")\n",
    "            df = df.dropna(subset=[0,4,5])\n",
    "            df = df[df[5] > df[4]]\n",
    "\n",
    "            # Apply strict unique scale mapping (no extra ordering)\n",
    "            scale_name = scale_dd.value or \"Chromatic (12-tone)\"\n",
    "            if not str(scale_name).startswith(\"Chromatic\"):\n",
    "                df[2] = map_unique_to_scale_ladder_strict(df[2], root_dd.value, scale_name)\n",
    "                print(f\"Applied unique ladder: {len(np.unique(df[2]))} unique pitches in '{root_dd.value} {scale_name}'\")\n",
    "\n",
    "            n_rows = len(df)\n",
    "            n_events_total = 2 * n_rows\n",
    "            print(f\"Notes: {n_rows:,}  |  MIDI events (on+off): {n_events_total:,}\")\n",
    "\n",
    "            MAX_EVENTS = 20_000_000\n",
    "            if n_events_total > MAX_EVENTS:\n",
    "                print(f\"Too many events (> {MAX_EVENTS:,}). Reduce data or raise MAX_EVENTS.\"); return\n",
    "\n",
    "            # Prepare MIDI\n",
    "            bpm = bpm_slider.value\n",
    "            tpb = tpb_slider.value\n",
    "            tempo = mido.midi.parse.tempo2bpm if False else mido.bpm2tempo(bpm)  # keep simple\n",
    "            mid = mido.MidiFile(type=1, ticks_per_beat=tpb)\n",
    "\n",
    "            # Track 0: tempo/meta\n",
    "            meta = mido.MidiTrack()\n",
    "            meta.append(MetaMessage('set_tempo', tempo=mido.bpm2tempo(bpm), time=0))\n",
    "            mid.tracks.append(meta)\n",
    "\n",
    "            def sec2ticks(s):\n",
    "                return int(round(s * tpb * (1_000_000 / mido.bpm2tempo(bpm))))\n",
    "\n",
    "            # Progress bars\n",
    "            trk_ids = sorted(df[0].astype(int).unique().tolist())\n",
    "            prog_tracks = widgets.IntProgress(min=0, max=len(trk_ids), value=0, description='Tracks')\n",
    "            avg_events = max(1, n_events_total // max(1,len(trk_ids)))\n",
    "            prog_events = widgets.IntProgress(min=0, max=avg_events, value=0, description='Events')\n",
    "            status = widgets.HTML(\"Starting…\")\n",
    "            display(prog_tracks, prog_events, status)\n",
    "\n",
    "            # Build tracks\n",
    "            for ti, trk_id in enumerate(trk_ids, start=1):\n",
    "                status.value = f\"<b>Track {ti}/{len(trk_ids)}</b> (ID {trk_id}) — preparing…\"\n",
    "                td = df[df[0] == trk_id].sort_values(4)\n",
    "\n",
    "                ch   = td[1].to_numpy(dtype=int)\n",
    "                note = td[2].to_numpy(dtype=int)\n",
    "                vel  = td[3].to_numpy(dtype=int)\n",
    "                t_on = td[4].to_numpy(dtype=float)\n",
    "                t_off= td[5].to_numpy(dtype=float)\n",
    "                bad = t_off <= t_on\n",
    "                if np.any(bad):\n",
    "                    t_off = t_off.copy(); t_off[bad] = t_on[bad] + 1e-4\n",
    "\n",
    "                on_ticks  = (t_on  * tpb * (1_000_000 / mido.bpm2tempo(bpm))).round().astype(int)\n",
    "                off_ticks = (t_off * tpb * (1_000_000 / mido.bpm2tempo(bpm))).round().astype(int)\n",
    "\n",
    "                n = len(td)\n",
    "                ev = np.empty((2*n, 5), dtype=int)\n",
    "                ev[0::2, 0] = on_ticks; ev[0::2, 1] = 1; ev[0::2, 2] = note; ev[0::2, 3] = vel; ev[0::2, 4] = ch\n",
    "                ev[1::2, 0] = off_ticks; ev[1::2, 1] = 0; ev[1::2, 2] = note; ev[1::2, 3] = 0;   ev[1::2, 4] = ch\n",
    "\n",
    "                order = np.lexsort((ev[:,1], ev[:,0]))\n",
    "                ev = ev[order]\n",
    "\n",
    "                prog_events.max = max(1, ev.shape[0])\n",
    "                prog_events.value = 0\n",
    "                status.value = f\"<b>Track {ti}/{len(trk_ids)}</b> — writing {ev.shape[0]:,} events…\"\n",
    "\n",
    "                tr = mido.MidiTrack(); mid.tracks.append(tr)\n",
    "                prev = 0\n",
    "                STEP = 5000\n",
    "                for i in range(ev.shape[0]):\n",
    "                    tick, onoff, nte, vvv, chh = ev[i]\n",
    "                    dt = tick - prev\n",
    "                    if dt < 0: dt = 0\n",
    "                    tr.append(mido.Message('note_on' if onoff == 1 else 'note_off',\n",
    "                                           note=int(nte), velocity=int(vvv),\n",
    "                                           time=int(dt), channel=int(chh)))\n",
    "                    prev = tick\n",
    "                    if (i % STEP) == 0:\n",
    "                        prog_events.value = i\n",
    "\n",
    "                prog_events.value = ev.shape[0]\n",
    "                prog_tracks.value = ti\n",
    "                status.value = f\"Finished Track {ti}/{len(trk_ids)}\"\n",
    "\n",
    "            out_name = (str(name).rsplit('.',1)[0] or 'visond') + '_ViSoND.mid'\n",
    "            mid.save(out_name)\n",
    "            status.value = f\"<b>Done.</b> Saved {out_name}\"\n",
    "            display(FileLink(out_name))\n",
    "            display(HTML(f'<a href=\"/files/{out_name}\" download>Download {out_name}</a>'))\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(str(e))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {type(e).__name__}: {e}\")\n",
    "\n",
    "# Init scales\n",
    "scale_dd.options = list(SCALE_INTERVALS.keys())\n",
    "scale_dd.value = \"Chromatic (12-tone)\"\n",
    "\n",
    "render_btn.on_click(render_midi)\n",
    "print(\"UI ready: upload or path → set options → Render MIDI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da0df9-15a9-4747-b789-da0f68968326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
